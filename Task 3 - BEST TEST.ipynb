{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fdcacb-0bd0-4480-bddf-87abc78590f8",
   "metadata": {},
   "source": [
    "# Task 3 - BEST TEST\n",
    "\n",
    "This code won't run without the BEST TEST, which is not included here. It would only take small adapatations to expand this to any multiple choice test that you'd like to give to the GPT API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf811db-1ea7-4bb1-adb9-dd79721ca9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.__version__='1.5.3', np.__version__='1.24.2', matplotlib.__version__='3.7.1'\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests.exceptions\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(f'{pd.__version__=}, {np.__version__=}, {matplotlib.__version__=}')\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23cf7a-5100-4a0f-a8cc-d8bc7df482e1",
   "metadata": {},
   "source": [
    "Edit the code below to wherever your API key lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "514d813e-fe53-479e-8807-6f935b7d53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key_path = 'api_key'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e140989a-5bcc-4014-b1f9-d526ed8d28e0",
   "metadata": {},
   "source": [
    "Models:\n",
    "\n",
    "- gpt-4-0314 : Snapshot of gpt-4 from March 14th 2023. 8k model. \\$0.0300 per 1k tokens (15x cost of 3.5). Max 8,192 tokens per request.\n",
    "- gpt-3.5-turbo : most powerful (current) model, costs \\$0.0020 per 1k tokens. Max 4,096 tokens per request. Current version is gpt-3.5-turbo-0301. Trained up to Sept 2021.\n",
    "- ada : fastest model, costs \\$0.0004 per 1k tokens (1/5 of 3.5). Max 2,049 tokens per request. Trained up to Oct 2019.\n",
    "- babbage : slightly slower than ada, but more nuanced. Costs \\$0.0005 per 1k tokens. Max 2,049 tokens per request. Trained up to Oct 2019.\n",
    "- curie : again slower, costs \\$0.0020 per 1k tokens (same price as 3.5). Max 2,049 tokens per request. Trained up to Oct 2019.\n",
    "- davinci : strongest 3.0 model, comparable to 3.5 turbo. Costs \\$0.0200 per 1k tokens (10x cost of 3.5). Max 2,049 tokens per request. Trained up to Oct 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03017dca-d8cd-40dc-8853-afb8f090389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions loaded. There are currently 20 questions.\n"
     ]
    }
   ],
   "source": [
    "with open('bst.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "    # Split the text into entries using regular expressions\n",
    "    entries = re.split(r'\\d+\\)', text)[1:]\n",
    "\n",
    "    # Strip each entry and add it to a list\n",
    "    question_list = [f'Question {i+1}: ' + entry.strip() for i, entry in enumerate(entries)]\n",
    "print(f'Questions loaded. There are currently {len(question_list)} questions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a7822-8794-4480-ba93-1a80b1143063",
   "metadata": {},
   "source": [
    "## Test Parameters\n",
    "\n",
    "Change the two cells below in order to determine which models are used and which testing parameters are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b190527-49f1-4b1e-a6ec-7e683f0465f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-3.5-turbo-0301', 'gpt-4-0314']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4aba7af-4b70-4ca6-91bf-8d75b0115ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [1,5,20]\n",
    "duplication_folds = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a0c93-bcc4-4446-aa76-b3004d6ba00b",
   "metadata": {},
   "source": [
    "## Useful Functions\n",
    "\n",
    "First function creates a `messages` list that will feed into the GPT API.\n",
    "\n",
    "Second function cleans some logic to handle errors when the API is busy or throwing errors. Also tries to make sure that the GPT returned data in a table, as requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57525048-43b9-4137-a38d-aa46b077c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages(prompts):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are physician in training taking an exam.'},\n",
    "        {'role': 'user', 'content': 'You are a physician studying internal medicine. As part of your training, you are studying good practices in blood banking. You will be presented with a multiple choice test. There may be as few as 1 question, or as many as 20 questions. Please format your answer as a csv that uses ; as the separator. Denote the start and end of your table with three tildes (~~~). Use `Question` and `Answer` as the columns in the table. In the `Question` column give only the number of the question. In the `Answer` column give your answer as a single letter corresponding to your choice in the multiple choice question.'},\n",
    "        {'role': 'assistant', \"content\": \"I understand. Please provide me with the questions.\",},\n",
    "        #{\n",
    "        #    'role': 'user',\n",
    "        #    'content': PROMPT_HERE\n",
    "        #},\n",
    "    ]\n",
    "    for p in prompts:\n",
    "        messages.append(\n",
    "                {\n",
    "            'role': 'user',\n",
    "            'content': p\n",
    "        }\n",
    "            )\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3602a73b-2723-4137-ad51-f94c4e40dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_response(response):\n",
    "    if response['choices'][0]['finish_reason'] != 'stop':\n",
    "        raise Exception('Response failed- message incomplete')\n",
    "            \n",
    "    if len(response['choices'][0]['message']['content'].split('~~~')) < 2:\n",
    "        print('\\nError occurred with response. Likely misformatted:')\n",
    "        print(response['choices'][0]['message']['content'])\n",
    "        raise Exception('Response failed- message misformatted')\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e8a37-9d2d-40e6-baee-17094e8c8515",
   "metadata": {},
   "source": [
    "## Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca07eea5-f542-4746-8e6d-bb63ed8606a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with model='gpt-3.5-turbo-0301', batch_size=1, duplication_folds=20\n",
      "********\n",
      "Unexpected error occurred: That model is currently overloaded with other requ. Retrying in 30 seconds...\n",
      "***\n",
      "Unexpected error occurred: That model is currently overloaded with other requ. Retrying in 30 seconds...\n",
      "*********\n",
      "Task completed on model='gpt-3.5-turbo-0301' and batch_size=1 with 128000 total tokens and a total cost of $0.25. Total time elapsed: 578.39s (9.64 minutes)\n",
      "Starting with model='gpt-3.5-turbo-0301', batch_size=5, duplication_folds=20\n",
      "********************\n",
      "Task completed on model='gpt-3.5-turbo-0301' and batch_size=5 with 70400 total tokens and a total cost of $0.14. Total time elapsed: 168.83s (2.81 minutes)\n",
      "Starting with model='gpt-3.5-turbo-0301', batch_size=20, duplication_folds=20\n",
      "********************\n",
      "Task completed on model='gpt-3.5-turbo-0301' and batch_size=20 with 59600 total tokens and a total cost of $0.11. Total time elapsed: 115.23s (1.92 minutes)\n",
      "Starting with model='gpt-4-0314', batch_size=1, duplication_folds=20\n",
      "********************\n",
      "Task completed on model='gpt-4-0314' and batch_size=1 with 126400 total tokens and a total cost of $3.78. Total time elapsed: 1271.16s (21.19 minutes)\n",
      "Starting with model='gpt-4-0314', batch_size=5, duplication_folds=20\n",
      "********************\n",
      "Task completed on model='gpt-4-0314' and batch_size=5 with 69760 total tokens and a total cost of $2.04. Total time elapsed: 434.92s (7.25 minutes)\n",
      "Starting with model='gpt-4-0314', batch_size=20, duplication_folds=20\n",
      "********************\n",
      "Task completed on model='gpt-4-0314' and batch_size=20 with 59140 total tokens and a total cost of $1.68. Total time elapsed: 324.39s (5.41 minutes)\n"
     ]
    }
   ],
   "source": [
    "results = {m: {b: [] for b in batch_sizes} for m in models}\n",
    "for model in models:\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f'Starting with {model=}, {batch_size=}, {duplication_folds=}')\n",
    "        \n",
    "        token_count = 0\n",
    "        start_time = time.time()\n",
    "        for fold in range(duplication_folds):\n",
    "            \n",
    "            #print(f\"Starting fold {fold}\", end=\" \")\n",
    "            \n",
    "            for i in range(0, len(question_list), batch_size):\n",
    "                thous_tokens = token_count // 1000\n",
    "                if 'gpt-4' in model:\n",
    "                    cost = 0.03 * thous_tokens\n",
    "                elif 'gpt-3.5' in model:\n",
    "                    cost = 0.002 * thous_tokens\n",
    "                else:\n",
    "                    cost = float('inf')\n",
    "                #print(f'{i=}, {thous_tokens=:.0f}k tokens ${cost:.3f}', end=' ')\n",
    "                \n",
    "                while True:\n",
    "                    try:\n",
    "                        response = openai.ChatCompletion.create(\n",
    "                            model=model,\n",
    "                            messages=get_messages(question_list[i:i+batch_size]),\n",
    "                            temperature=0,\n",
    "                        )\n",
    "                        if val_response(response):\n",
    "                            break\n",
    "                    except (\n",
    "                        openai.error.APIConnectionError,\n",
    "                        requests.exceptions.Timeout,\n",
    "                        requests.exceptions.ConnectionError,\n",
    "                        openai.error.APIError,\n",
    "                        openai.error.ServiceUnavailableError,\n",
    "                        TimeoutError\n",
    "                    ) as e:\n",
    "                        print(f\"\\nConnection error occurred: {str(e)[:50]}: Retrying in 30 seconds...\")\n",
    "                        time.sleep(30)                \n",
    "                    except Exception as e:\n",
    "                        print(f\"\\nUnexpected error occurred: {str(e)[:50]}. Retrying in 30 seconds...\")\n",
    "                        time.sleep(30)\n",
    "                        \n",
    "                token_count += response['usage']['total_tokens']\n",
    "                r = response['choices'][0]['message']['content'].split('~~~')[1].strip()\n",
    "                results[model][batch_size].append(pd.read_csv(StringIO(r), sep=';'))\n",
    "                #print(f\"completed. Tokens: {response['usage']['total_tokens']}, time elapsed: {time.time()-start_time:.2f}s\")\n",
    "            #print(f'Finished fold {fold}. Total time elapsed: {time.time()-start_time:.2f}s')\n",
    "            print('*',end='')\n",
    "        print(f'\\nTask completed on {model=} and {batch_size=} with {token_count} total tokens and a total cost of ${cost:.2f}.\\n\\tTotal time elapsed: {time.time()-start_time:.2f}s ({(time.time()-start_time)/60:.2f} minutes)\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d5da1e0-d1a1-42c1-bea1-4a4baa0198e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = pd.read_csv('key_bst.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "452e2077-52ac-4cce-90a8-76783c91db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "raw_dfs = []\n",
    "for bs in batch_sizes:\n",
    "    for m in models:\n",
    "        if 'gpt-3.5' in m:\n",
    "            test_name = f'GPT3.5_batch_{bs}'\n",
    "        elif 'gpt-4' in m:\n",
    "            test_name = f'GPT4_batch_{bs}'\n",
    "            \n",
    "        dummy = pd.concat(results[m][bs], ignore_index=True)\n",
    "        \n",
    "        # Merge dummy with key on the Question column\n",
    "        merged = pd.merge(dummy, key, on='Question')\n",
    "        \n",
    "        # Create a new column test_name that indicates whether the Answer matches the Key\n",
    "        merged[test_name] = merged['Answer'] == merged['Key']\n",
    "        \n",
    "        # Convert True/False values to 1/0\n",
    "        merged[test_name] = merged[test_name].astype(int)\n",
    "        \n",
    "        # Drop the 'Key' column and keep only the 'Question', 'Answer', and test_name columns\n",
    "        merged = merged[['Question', 'Answer', test_name]]\n",
    "        \n",
    "        raw_dfs.append(merged)\n",
    "        merged.to_csv(f'raw_{test_name}_results.csv')\n",
    "        \n",
    "        merged = merged.groupby(['Question']).agg({test_name: 'mean'}).reset_index()\n",
    "        \n",
    "        all_dfs.append(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70407dae-1546-478f-8acc-52d1332d6336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84db0a5-9cf6-485e-aeaf-53fe35407953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(all_dfs[0], all_dfs[1], on='Question')\n",
    "\n",
    "for i in range(2, len(all_dfs)):\n",
    "    df = pd.merge(df, all_dfs[i], on='Question')\n",
    "    \n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
